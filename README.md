# Understanding How Depth and Width Affect Multi-Layer Perceptron (MLP) Performance


## Overview

This repository contains the code, tutorial, and analysis for studying how depth (number of layers) and width (units per layer) affect the performance of a Multi-Layer Perceptron (MLP) classifier.

The project includes:

A complete Jupyter notebook for experiments

Plots comparing different architectures

A tutorial PDF explaining the theory and results

Source references and documentation

## Objectives
This project investigates:
How increasing width affects accuracy, training stability, and generalisation
Whether adding more layers improves MLP performance
Parameter efficiency vs. performance trade-offs
Training behaviour shown through loss and accuracy curves

## Contents

- Machine_Learning_Assignment.ipynb  
  Jupyter notebook with all TensorFlow/Keras code used to train the models and generate the plots.

- Understanding How Depth and Width Affect Multi-Layer Perceptron (MLP) Performance.pdf  
  PDF version of the tutorial

- plots/  
  Saved figures used in the tutorial.

## How to run

1. Install Python 3 and the required packages:

   ```bash
   pip install tensorflow matplotlibÂ numpy
